<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Dmitriy Selivanov" />

<meta name="date" content="2016-01-10" />

<title>GloVe word embeddings.</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css,body%20%7B%0A%20%20background%2Dcolor%3A%20%23fff%3B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20max%2Dwidth%3A%20700px%3B%0A%20%20overflow%3A%20visible%3B%0A%20%20padding%2Dleft%3A%202em%3B%0A%20%20padding%2Dright%3A%202em%3B%0A%20%20font%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0A%20%20font%2Dsize%3A%2014px%3B%0A%20%20line%2Dheight%3A%201%2E35%3B%0A%7D%0A%0A%23header%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0A%0A%23TOC%20%7B%0A%20%20clear%3A%20both%3B%0A%20%20margin%3A%200%200%2010px%2010px%3B%0A%20%20padding%3A%204px%3B%0A%20%20width%3A%20400px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20border%2Dradius%3A%205px%3B%0A%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20font%2Dsize%3A%2013px%3B%0A%20%20line%2Dheight%3A%201%2E3%3B%0A%7D%0A%20%20%23TOC%20%2Etoctitle%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%20%20font%2Dsize%3A%2015px%3B%0A%20%20%20%20margin%2Dleft%3A%205px%3B%0A%20%20%7D%0A%0A%20%20%23TOC%20ul%20%7B%0A%20%20%20%20padding%2Dleft%3A%2040px%3B%0A%20%20%20%20margin%2Dleft%3A%20%2D1%2E5em%3B%0A%20%20%20%20margin%2Dtop%3A%205px%3B%0A%20%20%20%20margin%2Dbottom%3A%205px%3B%0A%20%20%7D%0A%20%20%23TOC%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dleft%3A%20%2D2em%3B%0A%20%20%7D%0A%20%20%23TOC%20li%20%7B%0A%20%20%20%20line%2Dheight%3A%2016px%3B%0A%20%20%7D%0A%0Atable%20%7B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dcolor%3A%20%23DDDDDD%3B%0A%20%20border%2Dstyle%3A%20outset%3B%0A%20%20border%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0A%20%20border%2Dwidth%3A%202px%3B%0A%20%20padding%3A%205px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%20%20line%2Dheight%3A%2018px%3B%0A%20%20padding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0A%20%20border%2Dleft%2Dstyle%3A%20none%3B%0A%20%20border%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Ap%20%7B%0A%20%20margin%3A%200%2E5em%200%3B%0A%7D%0A%0Ablockquote%20%7B%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20padding%3A%200%2E25em%200%2E75em%3B%0A%7D%0A%0Ahr%20%7B%0A%20%20border%2Dstyle%3A%20solid%3B%0A%20%20border%3A%20none%3B%0A%20%20border%2Dtop%3A%201px%20solid%20%23777%3B%0A%20%20margin%3A%2028px%200%3B%0A%7D%0A%0Adl%20%7B%0A%20%20margin%2Dleft%3A%200%3B%0A%7D%0A%20%20dl%20dd%20%7B%0A%20%20%20%20margin%2Dbottom%3A%2013px%3B%0A%20%20%20%20margin%2Dleft%3A%2013px%3B%0A%20%20%7D%0A%20%20dl%20dt%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%7D%0A%0Aul%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%7D%0A%20%20ul%20li%20%7B%0A%20%20%20%20list%2Dstyle%3A%20circle%20outside%3B%0A%20%20%7D%0A%20%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dbottom%3A%200%3B%0A%20%20%7D%0A%0Apre%2C%20code%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20color%3A%20%23333%3B%0A%7D%0Apre%20%7B%0A%20%20white%2Dspace%3A%20pre%2Dwrap%3B%20%20%20%20%2F%2A%20Wrap%20long%20lines%20%2A%2F%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20margin%3A%205px%200px%2010px%200px%3B%0A%20%20padding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Acode%20%7B%0A%20%20font%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0A%20%20font%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0A%20%20padding%3A%202px%200px%3B%0A%7D%0A%0Adiv%2Efigure%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0A%20%20background%2Dcolor%3A%20%23FFFFFF%3B%0A%20%20padding%3A%202px%3B%0A%20%20border%3A%201px%20solid%20%23DDDDDD%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20margin%3A%200%205px%3B%0A%7D%0A%0Ah1%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%20%20font%2Dsize%3A%2035px%3B%0A%20%20line%2Dheight%3A%2040px%3B%0A%7D%0A%0Ah2%20%7B%0A%20%20border%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20padding%2Dbottom%3A%202px%3B%0A%20%20font%2Dsize%3A%20145%25%3B%0A%7D%0A%0Ah3%20%7B%0A%20%20border%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20font%2Dsize%3A%20120%25%3B%0A%7D%0A%0Ah4%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0A%20%20margin%2Dleft%3A%208px%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Ah5%2C%20h6%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23ccc%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Aa%20%7B%0A%20%20color%3A%20%230033dd%3B%0A%20%20text%2Ddecoration%3A%20none%3B%0A%7D%0A%20%20a%3Ahover%20%7B%0A%20%20%20%20color%3A%20%236666ff%3B%20%7D%0A%20%20a%3Avisited%20%7B%0A%20%20%20%20color%3A%20%23800080%3B%20%7D%0A%20%20a%3Avisited%3Ahover%20%7B%0A%20%20%20%20color%3A%20%23BB00BB%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%0A%2F%2A%20Class%20described%20in%20https%3A%2F%2Fbenjeffrey%2Ecom%2Fposts%2Fpandoc%2Dsyntax%2Dhighlighting%2Dcss%0A%20%20%20Colours%20from%20https%3A%2F%2Fgist%2Egithub%2Ecom%2Frobsimmons%2F1172277%20%2A%2F%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Keyword%20%2A%2F%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%2F%2A%20DataType%20%2A%2F%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%2F%2A%20DecVal%20%28decimal%20values%29%20%2A%2F%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20BaseN%20%2A%2F%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Float%20%2A%2F%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Char%20%2A%2F%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20String%20%2A%2F%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%2F%2A%20Comment%20%2A%2F%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%2F%2A%20OtherToken%20%2A%2F%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20AlertToken%20%2A%2F%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Function%20calls%20%2A%2F%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%2F%2A%20ErrorTok%20%2A%2F%0A%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">GloVe word embeddings.</h1>
<h4 class="author"><em>Dmitriy Selivanov</em></h4>
<h4 class="date"><em>2016-01-10</em></h4>
</div>


<div id="word-embeddings" class="section level1">
<h1>Word embeddings</h1>
<p>After Tomas Mikolov et al. released <a href="https://code.google.com/p/word2vec/">word2vec</a> tool, there was a boom of articles about words vector representations. One of the greatest is <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a>, which did a big thing while explaining how such algorithms work and refolmulating word2vec optimizations as special kind of factoriazation for word cooccurences matrix.</p>
<p>Here I will briefly introduce GloVe algorithm and show how to use its text2vec implementation.</p>
</div>
<div id="introduction-to-glove-algorithm" class="section level1">
<h1>Introduction to GloVe algorithm</h1>
<p>GloVe algorithm consists of following steps:</p>
<ol style="list-style-type: decimal">
<li>Collect word cooccurence statistics in a form of word coocurence matrix <span class="math">\(X\)</span>. Each element <span class="math">\(X_{ij}\)</span> of such matrix represents measure of how often <em>word i</em> appears in context of <em>word j</em>. Usually we scan our corpus in followinf manner: for each term we look for context terms withing some area - <em>window_size</em> before and <em>window_size</em> after. Also we give less weight for more distand words. Usually <span class="math">\[decay = 1/offset\]</span>.</li>
<li>Define soft constraint for each word pair: <span class="math">\[w_i^Tw_j + b_i + b_j = log(X_{ij})\]</span> Here <span class="math">\(w_i\)</span> - vector for main word, <span class="math">\(w_j\)</span> - vector for context word, <span class="math">\(b_i\)</span>, <span class="math">\(b_j\)</span> - scalar biases for main and context words.</li>
<li>Define cost function <span class="math">\[J = \sum_{i=1}^V \sum_{j=1}^V \; f(X_{ij}) ( w_i^T w_j + b_i + b_j - \log X_{ij})^2\]</span> Here <span class="math">\(f\)</span> is a weighting function which help us to prevent learning only from exremly common word pairs. GloVe authors choose following fucntion:</li>
</ol>
<p><span class="math">\[
f(X_{ij}) = 
\begin{cases}
(\frac{X_{ij}}{x_{max}})^\alpha &amp; \text{if } X_{ij} &lt; XMAX \\
1 &amp; \text{otherwise}
\end{cases}
\]</span></p>
</div>
<div id="canonical-example---linguistic-regularities" class="section level1">
<h1>Canonical example - linguistic regularities</h1>
<p>Now lets examine how it works. As commonly known word2vec word vectors capture many linguistic regularities. The most canonical example is following. If we will take word vectors for words <em>paris, france, italy</em> and perform following operation: <span class="math">\[vector('paris') - vector('france') + vector('italy')\]</span> resultiong vector will be close to <span class="math">\(vector('rome')\)</span>.</p>
<p>Lets download some wikipedia data (same data used in ./demo-word.sh in word2vec):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(text2vec)
<span class="kw">library</span>(readr)
temp &lt;-<span class="st"> </span><span class="kw">tempfile</span>()
<span class="kw">download.file</span>(<span class="st">'http://mattmahoney.net/dc/text8.zip'</span>, temp)
wiki &lt;-<span class="st"> </span><span class="kw">read_lines</span>(<span class="kw">unz</span>(temp, <span class="st">&quot;text8&quot;</span>))
<span class="kw">unlink</span>(temp)</code></pre>
<p>In the next step we will create vocabulary - set of words for which we want to learn word vectors. Note, that all text2vec’s functions that operates on raw text data (<code>vocabulary</code>, <code>create_hash_corpus</code>, <code>create_vocab_corpus</code>) have streaming API and you should iterator over tokens as first argument for these functions.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create iterator over tokens</span>
it &lt;-<span class="st"> </span><span class="kw">itoken</span>(wiki, 
             <span class="co"># text is already pre-cleaned</span>
             <span class="dt">preprocess_function =</span> identity, 
             <span class="co"># all words are single whitespace separated</span>
             <span class="dt">tokenizer =</span> function(x) <span class="kw">strsplit</span>(x, <span class="dt">split =</span> <span class="st">&quot; &quot;</span>, <span class="dt">fixed =</span> T))
<span class="co"># create vocabulary. Terms will be unigrams (simple words).</span>
vocab &lt;-<span class="st"> </span><span class="kw">vocabulary</span>(it, <span class="dt">ngram =</span> <span class="kw">c</span>(1L, 1L) )</code></pre>
<p>These words should not be too rare. Fot example we will can’t obtain any meaningful word vector for word which we saw only once in entire corpus. Here we will take only words which appear at least 5 times. <em>text2vec</em> provides more options to filter vocabulary - see <code>?prune_vocabulary</code> function.</p>
<pre class="sourceCode r"><code class="sourceCode r">vocab &lt;-<span class="st"> </span><span class="kw">prune_vocabulary</span>(vocab, <span class="dt">term_count_min =</span> <span class="dv">5</span>)</code></pre>
<p>Now we have <code>71290</code> terms in vocalulary and ready to construct Term-Coocurence matrix (<em>tcm</em>).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># as said above, we should provide iterator to create_vocab_corpus function</span>
it &lt;-<span class="st"> </span><span class="kw">itoken</span>(wiki, 
             <span class="co"># text is already pre-cleaned</span>
             <span class="dt">preprocess_function =</span> identity, 
             <span class="co"># all words are single whitespace separated</span>
             <span class="dt">tokenizer =</span> function(x) <span class="kw">strsplit</span>(x, <span class="dt">split =</span> <span class="st">&quot; &quot;</span>, <span class="dt">fixed =</span> T))

corpus &lt;-<span class="st"> </span><span class="kw">create_vocab_corpus</span>(it, 
                              <span class="co"># use our filtered vocabulary</span>
                              <span class="dt">vocabulary =</span> vocab,
                              <span class="co"># don't create document-term matrix</span>
                              <span class="dt">grow_dtm =</span> F,
                              <span class="co"># use window of 5 for context words</span>
                              <span class="dt">skip_grams_window =</span> 15L)

<span class="co"># get term cooccurence matrix from instance of C++ corpus class</span>
tcm &lt;-<span class="st"> </span><span class="kw">get_tcm</span>(corpus)</code></pre>
<p>Now we have <em>tcm</em> matrix and can factorize it via GloVe algorithm.<br />text2vec uses parallel stochastic gradient descend algorithm. By default it use all cores on your machine, but you can specify number of core directly. For example for using 4 threads, call <code>RcppParallel::setThreadOptions(numThreads = 4)</code>.</p>
<p>Finally lets fit our model (it can take several of minutes to fit!):</p>
<pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">glove</span>(<span class="dt">tcm =</span> tcm,
             <span class="dt">word_vectors_size =</span> <span class="dv">50</span>,
             <span class="dt">x_max =</span> <span class="dv">10</span>, <span class="dt">learning_rate =</span> <span class="fl">0.2</span>,
             <span class="dt">num_iters =</span> <span class="dv">15</span>)</code></pre>
<blockquote>
<p>2016-01-10 14:12:37 - epoch 1, expected cost 0.0662<br />2016-01-10 14:12:51 - epoch 2, expected cost 0.0472<br />2016-01-10 14:13:06 - epoch 3, expected cost 0.0429<br />2016-01-10 14:13:21 - epoch 4, expected cost 0.0406<br />2016-01-10 14:13:36 - epoch 5, expected cost 0.0391<br />2016-01-10 14:13:50 - epoch 6, expected cost 0.0381<br />2016-01-10 14:14:05 - epoch 7, expected cost 0.0373<br />2016-01-10 14:14:19 - epoch 8, expected cost 0.0366<br />2016-01-10 14:14:33 - epoch 9, expected cost 0.0362<br />2016-01-10 14:14:47 - epoch 10, expected cost 0.0358<br />2016-01-10 14:15:01 - epoch 11, expected cost 0.0355<br />2016-01-10 14:15:16 - epoch 12, expected cost 0.0351<br />2016-01-10 14:15:30 - epoch 13, expected cost 0.0349<br />2016-01-10 14:15:44 - epoch 14, expected cost 0.0347<br />2016-01-10 14:15:59 - epoch 15, expected cost 0.0345</p>
</blockquote>
<p>And obtain word vectors</p>
<pre class="sourceCode r"><code class="sourceCode r">word_vectors &lt;-<span class="st"> </span>fit$word_vectors[[<span class="dv">1</span>]] +<span class="st"> </span>fit$word_vectors[[<span class="dv">2</span>]]
<span class="kw">rownames</span>(word_vectors) &lt;-<span class="st"> </span><span class="kw">rownames</span>(tcm)</code></pre>
<p>Find closest word vectors for our <em>paris - france + italy</em> example:</p>
<pre class="sourceCode r"><code class="sourceCode r">word_vectors_norm &lt;-<span class="st">  </span><span class="kw">sqrt</span>(<span class="kw">rowSums</span>(word_vectors ^<span class="st"> </span><span class="dv">2</span>))

rome &lt;-<span class="st"> </span>word_vectors[<span class="st">'paris'</span>, , drop =<span class="st"> </span>F] -<span class="st"> </span>
<span class="st">  </span>word_vectors[<span class="st">'france'</span>, , drop =<span class="st"> </span>F] +<span class="st"> </span>
<span class="st">  </span>word_vectors[<span class="st">'italy'</span>, , drop =<span class="st"> </span>F]

cos_dist &lt;-<span class="st"> </span>text2vec:::<span class="kw">cosine</span>(rome, 
                              word_vectors, 
                              word_vectors_norm)
<span class="kw">head</span>(<span class="kw">sort</span>(cos_dist[<span class="dv">1</span>,], <span class="dt">decreasing =</span> T), <span class="dv">5</span>)
##    paris    venice     genoa      rome  florence
##0.7811252 0.7763088 0.7048109 0.6696540 0.6580989</code></pre>
<p>You can achieve <strong>much</strong> better results by experimenting with <code>skip_grams_window</code> and parameters of <code>glove()</code> function (word vectors size, number of iterations, etc.). For more details and large-scale experiments on wikipedia data see this <a href="http://dsnotes.com/blog/text2vec/2015/12/01/glove-enwiki/">post</a> in my blog.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
