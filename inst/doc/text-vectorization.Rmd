---
title: "Analyzing ~~large amounts of~~ texts with text2vec package."
author: "Dmitriy Selivanov"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analyzing texts with text2vec package.}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Features

**text2vec** is a package for which the main goal is to provide an **efficient framework** with **concise API** for **text analysis** and **natural language processing (NLP)** in R. 

## Core functionality

At the moment we cover two following topics:  

1. Fast text vectorization on arbitrary n-grams.
    - using vocabulary
    - using feature hashing
2. State-of-the-art [GloVe](http://www-nlp.stanford.edu/projects/glove/) word embeddings.

## Efficiency  

- The core of the functionality is **carefully written in C++**. Also this means text2vec is **memory friendly**.
- Some parts (GloVe training) are fully **parallelized** using an excellent [RcppParallel](http://rcppcore.github.io/RcppParallel/) package. This means, **parallel features work on OS X, Linux, Windows and Solaris(x86) without any additinal tuning/hacking/tricks**.
- **Streaming API**, this means users don't have to load all the data into RAM. **text2vec** allows processing streams of chunks.


# Text vectorization

Historically, most of the text-mining and NLP modelling was related to [Bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) or [Bag-of-ngrams](https://en.wikipedia.org/wiki/N-gram) models. Despite of simplicity, these models usually demonstrates good performance on text categorization/classification tasks. But, in contrast to theoretical simplicity and practical efficiency, building *bag-of-words* models involves technical challenges. Especially within `R` framework, because of its typical copy-on-modify semantics. 

## Pipeline 

Lets briefly review some details of typical analysis pipeline:  

1. Usually reseacher have to construct [Document-Term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) (DTM) from imput documents. Or in other words, **vectorize text** - create mapping from words/ngrams to [vector space](https://en.wikipedia.org/wiki/Vector_space_model).
1. Fit model on this DTM. This can include:
    - text classification
    - topic modeling
    - ...
1. Tune, validate model.
1. Apply model on new data.

Here we will discuss mostly first stage. Underlying texts can take a lot of space, but vectorized ones usually not, because they are stored in form of sparse matrices. In R it is not very easy (from reason above - copy-on-modify semantics) to iteratively grow DTM. So construction of such objects, even for small collections of documents, can become serious hedache for analysts and researchers. It involves reading the whole collection of text documents into RAM and process it as single vector, which easily increase memory consumption by factor of 2 to 4 (to tell the truth, this is quite optimistically). Fortunately, there is a better, text2vec way. Lets check how it works on simple example.

## Sentiment analysis on IMDB moview review dataset
**text2vec** provides `movie_review` dataset. It consists of 5000 movie review, each of which marked ad positive or negative.
```{r, loading-data, eval=TRUE}
library(text2vec)
data("movie_review")
set.seed(42L)
```
To represent documents in vector space, first of all we have to create `term -> term_id` mappings. We use termin *term* instead of *word*, because actually it can be arbitrary *ngram*, not just single word. Having set of documents we want represent them as *sparse matrix*, where each row should corresponds to *document* and each column should corresponds to *term*. This can be done in 2 ways: using **vocabulary**, or by **feature hashing** (hashing trick).

### Vocabulary based vectorization
Lets examine the first choice. He we collect unique terms from all documents and mark them with *unique_id*. `vocabulary()` function designed specially for this purpose.
```{r, vocab-iterator, eval=TRUE}
it <- itoken(movie_review[['review']], preprocess_function = tolower, 
             tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
# using unigrams here
t1 <- Sys.time()
vocab <- vocabulary(src = it, ngram = c(1L, 1L))
print( difftime( Sys.time(), t1, units = 'sec'))
```
Now we can costruct Document-Term Matrix (DTM). Again, since all functions related to *corpus* construction have streaming API, we have to create *iterator* and provide it to `create_vocab_corpus` function:
```{r, vocab_dtm_1, eval=TRUE}
it <- itoken(movie_review[['review']], preprocess_function = tolower, 
             tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
corpus <- create_vocab_corpus(it, vocabulary = vocab)
dtm <- get_dtm(corpus)
```
We got DTM matrix. Lets check its dimension:
```{r, vocab_dtm_1_dim, eval=TRUE}
dim(dtm)
```
As you can see, it has `r dim(dtm)[[1]]` rows (equal to number of documents) and `r dim(dtm)[[2]]` columns (equal to number of unique terms).
Now we are ready to fit our first model. Here we will use `glmnet` package to fit *logistic regression* with *L1* penalty.
```{r, fit_1, message=FALSE, warning=FALSE, eval=TRUE}
library(glmnet)
t1 <- Sys.time()
fit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], 
                 family = 'binomial', 
                 # lasso penalty
                 alpha = 1,
                 # interested area unded ROC curve
                 type.measure = "auc",
                 # 5-fold cross-validation
                 nfolds = 5,
                 # high value, less accurate, but faster training
                 thresh = 1e-3,
                 # again lower number iterations for faster training
                 # in this vignette
                 maxit = 1e3)
print( difftime( Sys.time(), t1, units = 'sec'))
plot(fit)
print (paste("max AUC = ", round(max(fit$cvm), 4)))
```

Note, that training time is quite high. We can reduce it and also significantly improve accuracy.

### Pruning vocabulary

We will prune our vocabulary. For example we can find words *"a"*, *"the"*, *"in"* in almost all documents, but actually they don't give any useful information. Usually they called [stop words](https://en.wikipedia.org/wiki/Stop_words). But in contrast to them, corpus also contains very *uncommon terms*, which contained only in few documents. These terms also useless, because we don't have sufficient statistics for them. Here we will filter them out:

```{r, prune_vocab_dtm_1}
# remove very common and uncommon words
pruned_vocab <- prune_vocabulary(vocab, term_count_min = 10,
 doc_proportion_max = 0.5, doc_proportion_min = 0.001)

it <- itoken(movie_review[['review']], preprocess_function = tolower, 
             tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
corpus <- create_vocab_corpus(it, vocabulary = pruned_vocab)
dtm <- get_dtm(corpus)
```

###  TF-IDF

Also we can (and usually should!) apply **[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) transofrmation**, which will increase weight for document-specific terms and decrease weight for widely used terms:

```{r, tfidf_dtm_1}
dtm <- dtm %>% tfidf_transformer
dim(dtm)
```

Now, lets fit out model again:
```{r, fit_2, message=FALSE, warning=FALSE, eval=TRUE}
t1 <- Sys.time()
fit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], 
                 family = 'binomial', 
                 # lasso penalty
                 alpha = 1,
                 # interested area unded ROC curve
                 type.measure = "auc",
                 # 5-fold cross-validation
                 nfolds = 5,
                 # high value, less accurate, but faster training
                 thresh = 1e-3,
                 # again lower number iterations for faster training
                 # in this vignette
                 maxit = 1e3)
print( difftime( Sys.time(), t1, units = 'sec'))
plot(fit)
print (paste("max AUC = ", round(max(fit$cvm), 4)))
```

As you can seem we obtain faster training, and larger AUC.

### Can we do better?

Also we can try to use [ngram](https://en.wikipedia.org/wiki/N-gram)s instead of words.
We will use up to 3-ngrams:

```{r, ngram_dtm_1}
it <- itoken(movie_review[['review']], preprocess_function = tolower, 
             tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)

t1 <- Sys.time()
vocab <- vocabulary(src = it, ngram = c(1L, 3L))

print( difftime( Sys.time(), t1, units = 'sec'))

vocab <- vocab %>% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.5, doc_proportion_min = 0.001)

it <- itoken(movie_review[['review']], preprocess_function = tolower, 
             tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)

corpus <- create_vocab_corpus(it, vocabulary = vocab)

print( difftime( Sys.time(), t1, units = 'sec'))

dtm <- corpus %>% 
  get_dtm %>% 
  tfidf_transformer


dim(dtm)

t1 <- Sys.time()
fit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], 
                 family = 'binomial', 
                 # lasso penalty
                 alpha = 1,
                 # interested area unded ROC curve
                 type.measure = "auc",
                 # 5-fold cross-validation
                 nfolds = 5,
                 # high value, less accurate, but faster training
                 thresh = 1e-3,
                 # again lower number iterations for faster training
                 # in this vignette
                 maxit = 1e3)
print( difftime( Sys.time(), t1, units = 'sec'))
plot(fit)
print (paste("max AUC = ", round(max(fit$cvm), 4)))
```

So improved our model a little bit more. I'm leaving further tuning for the reader.

### Feature hashing

If you didn't hear anything about **Feature hashing** (or **hashing trick**), I recommend to start with [wikipedia article](https://en.wikipedia.org/wiki/Feature_hashing) and after that review [original paper](http://alex.smola.org/papers/2009/Weinbergeretal09.pdf) by Yahoo! research team. This techique is very fast - we don't perform look up over associative array. But another benefit is very low memory footprint - we can map arbitrary number of features into much more compact space. This method was popularized by Yahoo and widely used in [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/). 

Here I will demonstrate, how to use feature hashing in **text2vec**:

```{r, hash_dtm}
t1 <- Sys.time()

it <- itoken(movie_review[['review']], preprocess_function = tolower, 
             tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)

fh <- feature_hasher(hash_size = 2**16, ngram = c(1L, 3L))

corpus <- create_hash_corpus(it, feature_hasher = fh)
print( difftime( Sys.time(), t1, units = 'sec'))

dtm <- corpus %>% 
  get_dtm %>% 
  tfidf_transformer

dim(dtm)

t1 <- Sys.time()
fit <- cv.glmnet(x = dtm, y = movie_review[['sentiment']], 
                 family = 'binomial', 
                 # lasso penalty
                 alpha = 1,
                 # interested area unded ROC curve
                 type.measure = "auc",
                 # 5-fold cross-validation
                 nfolds = 5,
                 # high value, less accurate, but faster training
                 thresh = 1e-3,
                 # again lower number iterations for faster training
                 # in this vignette
                 maxit = 1e3)
print( difftime( Sys.time(), t1, units = 'sec'))
plot(fit)
print (paste("max AUC = ", round(max(fit$cvm), 4)))
```

As you can see, we got a little bit worse AUC, but DTM construction time was considerably lower. On large collections of documents this can become a serious argument.
